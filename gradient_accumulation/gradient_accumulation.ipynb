{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Create a simple CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Generate a random dataset\n",
    "num_samples = 6400\n",
    "num_features = 1\n",
    "height = 28\n",
    "width = 28\n",
    "images = torch.randn(num_samples, num_features, height, width)\n",
    "labels = torch.randint(0, 2, (num_samples,))\n",
    "\n",
    "def train_without_gradient_accumulation():\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = SimpleCNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Training parameters\n",
    "    num_epochs = 10\n",
    "    batch_size = 64\n",
    "\n",
    "    # Training loop without gradient accumulation\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            inputs = images[i:i+batch_size]\n",
    "            target = labels[i:i+batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, target)\n",
    "\n",
    "            # Backward pass and update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print the average loss for this epoch\n",
    "        print(f'Epoch {epoch + 1}, Loss: {running_loss / (num_samples // batch_size)}')\n",
    "\n",
    "def train_with_gradient_accumulation():\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = SimpleCNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Training parameters\n",
    "    num_epochs = 10\n",
    "    desired_batch_size = 256\n",
    "    sub_batch_size = 64\n",
    "    accumulation_steps = desired_batch_size // sub_batch_size\n",
    "\n",
    "    # Training loop with gradient accumulation\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i in range(0, num_samples, desired_batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for j in range(i, i + desired_batch_size, sub_batch_size):\n",
    "                inputs = images[j:j+sub_batch_size]\n",
    "                target = labels[j:j+sub_batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, target)\n",
    "\n",
    "                # Backward pass with gradient accumulation\n",
    "                loss.backward()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # Update the model parameters after accumulating gradients\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print the average loss for this epoch\n",
    "        print(f'Epoch {epoch + 1}, Loss: {running_loss / (num_samples // desired_batch_size)}')\n",
    "\n",
    "\n",
    "# Function to calculate the average CPU usage during training\n",
    "def measure_cpu_usage(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        cpu_percentages = []\n",
    "        start_time = time.time()\n",
    "        for _ in range(10):  # Measure CPU usage 10 times per second\n",
    "            cpu_percentages.append(psutil.cpu_percent())\n",
    "            time.sleep(0.1)\n",
    "        avg_cpu_usage = sum(cpu_percentages) / len(cpu_percentages)\n",
    "        print(f'Average CPU usage during training: {avg_cpu_usage}%')\n",
    "        func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f'Training time: {end_time - start_time} seconds')\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training without gradient accumulation:\n",
      "Average CPU usage during training: 25.82%\n",
      "Epoch 1, Loss: 0.6937218672037124\n",
      "Epoch 2, Loss: 0.6932226049900055\n",
      "Epoch 3, Loss: 0.693155956864357\n",
      "Epoch 4, Loss: 0.6931175756454467\n",
      "Epoch 5, Loss: 0.693082891702652\n",
      "Epoch 6, Loss: 0.6930537974834442\n",
      "Epoch 7, Loss: 0.6930223977565766\n",
      "Epoch 8, Loss: 0.6929922187328339\n",
      "Epoch 9, Loss: 0.6929661071300507\n",
      "Epoch 10, Loss: 0.6929388135671616\n",
      "Training time: 8.093447923660278 seconds\n",
      "\n",
      "Training with gradient accumulation:\n",
      "Average CPU usage during training: 38.9%\n",
      "Epoch 1, Loss: 2.7724686956405638\n",
      "Epoch 2, Loss: 2.7722557759284974\n",
      "Epoch 3, Loss: 2.772132396697998\n",
      "Epoch 4, Loss: 2.7720070576667784\n",
      "Epoch 5, Loss: 2.7718866276741028\n",
      "Epoch 6, Loss: 2.7717677211761473\n",
      "Epoch 7, Loss: 2.771643841266632\n",
      "Epoch 8, Loss: 2.7715102553367617\n",
      "Epoch 9, Loss: 2.7713914942741393\n",
      "Epoch 10, Loss: 2.771271359920502\n",
      "Training time: 8.04003095626831 seconds\n"
     ]
    }
   ],
   "source": [
    "# Wrap the training functions with the measure_cpu_usage decorator\n",
    "train_without_gradient_accumulation = measure_cpu_usage(train_without_gradient_accumulation)\n",
    "train_with_gradient_accumulation = measure_cpu_usage(train_with_gradient_accumulation)\n",
    "\n",
    "# Run the training loops\n",
    "print('Training without gradient accumulation:')\n",
    "train_without_gradient_accumulation()\n",
    "print('\\nTraining with gradient accumulation:')\n",
    "train_with_gradient_accumulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
